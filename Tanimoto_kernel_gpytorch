# -*- coding: utf-8 -*-
"""
Created on Thu Apr 15 09:23:59 2021

@author: manoj
"""

import torch
import gpytorch
import pandas as pd
from rdkit.Chem import AllChem, MolFromSmiles
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import warnings
#from property_prediction.data_utils import TaskDataLoader


import os
smoke_test = ('CI' in os.environ)
training_iter = 2 if smoke_test else 50
torch.set_printoptions(precision=10)

#============================DATA==============================================
# importing the photoswith data into pandas dataframe and labeling out the inputs
df = pd.read_csv('photoswitches.csv')
X_s1 = df['SMILES'].to_list()
y_s1 = df['E isomer pi-pi* wavelength in nm'].to_numpy()

# Delete NaN values
X_s = list(np.delete(np.array(X_s1), np.argwhere(np.isnan(y_s1))))
y_s = np.delete(y_s1, np.argwhere(np.isnan(y_s1)))

# Conversion of molecular SMILES to morganfingerprint
rdkit_mols = [MolFromSmiles(smiles) for smiles in X_s]
X = [AllChem.GetMorganFingerprintAsBitVect(mol, 3, 2048) for mol in rdkit_mols]
X = np.asarray(X)

# split the data with test_size of 0.2 split
X_train, X_test, y_train, y_test = train_test_split(X, y_s, test_size=0.2, random_state= None)
y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)


def transform_data(X_train, y_train, X_test, y_test, n_components=None, use_pca=False):
    """
    Apply feature scaling, dimensionality reduction to the data. Return the standardised and low-dimensional train and
    test sets together with the scaler object for the target values.

    :param X_train: input train data
    :param y_train: train labels
    :param X_test: input test data
    :param y_test: test labels
    :param n_components: number of principal components to keep when use_pca = True
    :param use_pca: Whether or not to use PCA
    :return: X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, y_scaler
    """

    x_scaler = StandardScaler()
    X_train_scaled = x_scaler.fit_transform(X_train)
    X_test_scaled = x_scaler.transform(X_test)
    y_scaler = StandardScaler()
    y_train_scaled = y_scaler.fit_transform(y_train)
    y_test_scaled = y_scaler.transform(y_test)

    if use_pca:
        pca = PCA(n_components)
        X_train_scaled = pca.fit_transform(X_train)
        print('Fraction of variance retained is: ' + str(sum(pca.explained_variance_ratio_)))
        X_test_scaled = pca.transform(X_test)

    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, y_scaler

#  We standardise the outputs but leave the inputs unchanged
_, y_train, _, y_test, y_scaler = transform_data(X_train, y_train, X_test, y_test)

#  Converting train test values into Tensor's
train_X = torch.Tensor(X_train)
train_y = torch.Tensor(y_train.reshape(-1,))
test_X = torch.Tensor(X_test)
test_y = torch.Tensor(y_test)


# ============================Tanimoto Kerenl implementation=========================================

def broadcasting_elementwise(op, a, b):
    
    # Apply binary operation `op` to every pair in tensors `a` and `b`.

    # :param op: binary operator on tensors, e.g. torch.add, torch.substract
    # :param a: torch.Tensor, shape [n_1, ..., n_a]
    # :param b: torch.Tensor, shape [m_1, ..., m_b]
    # :return: torch.Tensor, shape [n_1, ..., n_a, m_1, ..., m_b]
    
    flatres = op(torch.reshape(a, [-1, 1]), torch.reshape(b, [1, -1]))
    return flatres


# import positivity constaint
from gpytorch.constraints import Positive

class TanimotoKernel(gpytorch.kernels.Kernel):
    # the tanimoto kernel is stationary
    is_stationary = True
    
    # we register the parameter when initializing the kernel
    def __init__(self, num_dimensions = None, offset_prior=None, variance_prior = None, variance_constraint = None, **kwargs):
        super().__init__(**kwargs)
        
        # registe the raw parameter
        self.register_parameter(name="raw_variance", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))
   
        # set the parameter contraint to be positive, when nothing is specified
        if variance_constraint is None:
            variance_constraint = Positive()
            
        # register the constraint
        self.register_constraint("raw_variance", variance_constraint)
        
        #set the parameter prior
        if variance_prior is not None:            
            self.register_prior("variance_prior", variance_prior, lambda m: m.variance, lambda m, v: m._set_variance(v))
        
        if num_dimensions is not None:
            # Remove after 1.0
            warnings.warn("The `num_dimensions` argument is deprecated and no longer used.", DeprecationWarning)
            self.register_parameter(name="offset", parameter=torch.nn.Parameter(torch.zeros(1, 1, num_dimensions)))
        
        if offset_prior is not None:
            # Remove after 1.0
            warnings.warn("The `offset_prior` argument is deprecated and no longer used.", DeprecationWarning)
        self.register_parameter(name="raw_variance", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))
    
      # now set the actual parameter
    @property
    def variance(self):
        return self.raw_variance_constraint.transform(self.raw_variance)
        
    @variance.setter
    def variance(self, value):
        return self._set_variance(value)
        
    def _set_variance(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.raw_variance)
        # transform the actual value to a raw one by applying inverse transform
        self.initialize(raw_variance = self.raw_length_constraint.inverse_transform(value))            
    
    # define the kernel function
    def forward(self, X, X2 = None, last_dim_is_batch=False):
    
        if X2 is None:
            X2 = X
        Xs = torch.sum(torch.square(X), dim = -1)
        X2s = torch.sum(torch.square(X2), dim = -1)
        cross_product = torch.tensordot(X, X2, ([-1], [-1]))
    
        denominator = -cross_product + broadcasting_elementwise(torch.add, Xs, X2s)
    
        return self.variance * cross_product / denominator
 
# ========================Gaussian_Process_model========================================================
    
# Use the simplest form of GP model, exact inference
class TanimoGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_X, train_y, likelihood):
        super().__init__(train_X, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = TanimotoKernel()

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# initialize likelihood and model
likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint= gpytorch.constraints.GreaterThan(1e-4))
model = TanimoGPModel(train_X, train_y, likelihood)

# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
training_iter = 300
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

for i in range(training_iter):
    # Zero gradients from previous iteration
    optimizer.zero_grad()
    # Output from model
    output = model(torch.Tensor(train_X))
    # Calc loss and backprop gradients
    loss = -mll(output, torch.Tensor(train_y.reshape(-1,)))
    loss.backward()
    print('Iter %d/%d - Loss: %.3f   variance: %.3f   noise: %.3f' % (
        i + 1, training_iter, loss.item(),
        model.covar_module.variance.item(),
        model.likelihood.noise.item()
    ))
    optimizer.step()
 
# =================================GP_for_Predictions=================================================    
# gp for predictions
model.eval()
likelihood.eval()


with torch.no_grad():
    observed_pred = likelihood(model(test_X))
    
Z_t = observed_pred.mean.numpy().reshape(-1,1)
y_pred = y_scaler.inverse_transform(Z_t)
y_test1 = y_scaler.inverse_transform(y_test)

score = r2_score(y_test1, y_pred)
rmse = np.sqrt(mean_squared_error(y_test1, y_pred))
mae = mean_absolute_error(y_test1, y_pred)

print("\nR^2: {:.3f}".format(score))
print("RMSE: {:.3f}".format(rmse))
print("MAE: {:.3f}".format(mae))
    


    
