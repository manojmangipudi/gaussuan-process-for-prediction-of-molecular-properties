# -*- coding: utf-8 -*-
"""
Created on Thu Apr 15 09:23:59 2021

@author: manoj
"""

import math
import torch
import gpytorch
from matplotlib import pyplot as plt
import pandas as pd
from rdkit.Chem import AllChem, Descriptors, MolFromSmiles
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import tensorflow as tf
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import warnings
from property_prediction.data_utils import transform_data, TaskDataLoader, featurise_mols


import os
smoke_test = ('CI' in os.environ)
training_iter = 2 if smoke_test else 50
torch.set_printoptions(precision=10)

#============================DATA==============================================
# Training data is 100 points in [0,1] inclusive regularly spaced
df = pd.read_csv('photoswitches.csv')
X_s1 = df['SMILES'].to_list()
y_s1 = df['E isomer pi-pi* wavelength in nm'].to_numpy()

# Delete NaN values
X_s = list(np.delete(np.array(X_s1), np.argwhere(np.isnan(y_s1))))
y_s = np.delete(y_s1, np.argwhere(np.isnan(y_s1)))

X = featurise_mols(X_s, 'fingerprints')

X_train, X_test, y_train, y_test = train_test_split(X, y_s, test_size=0.2)
y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)

#  We standardise the outputs but leave the inputs unchanged
_, y_train, _, y_test, y_scaler = transform_data(X_train, y_train, X_test, y_test)

#  tensor conversion
train_X = torch.Tensor(X_train)
train_y = torch.Tensor(y_train.reshape(-1,))
test_X = torch.Tensor(X_test)
test_y = torch.Tensor(y_test)



# =================Kerenl implementation=========================================

def broadcasting_elementwise(op, a, b):
    
    # Apply binary operation `op` to every pair in tensors `a` and `b`.

    # :param op: binary operator on tensors, e.g. torch.add, torch.substract
    # :param a: torch.Tensor, shape [n_1, ..., n_a]
    # :param b: torch.Tensor, shape [m_1, ..., m_b]
    # :return: torch.Tensor, shape [n_1, ..., n_a, m_1, ..., m_b]
    
    flatres = op(torch.reshape(a, [-1, 1]), torch.reshape(b, [1, -1]))
    return flatres


# import positivity constaint
from gpytorch.constraints import Positive

class TanimotoKernel(gpytorch.kernels.Kernel):
    # the tanimoto kernel is stationary
    is_stationary = True
    
    # we register the parameter when initializing the kernel
    def __init__(self, num_dimensions = None, offset_prior=None, variance_prior = None, variance_constraint = None, **kwargs):
        super().__init__(**kwargs)
        
        # registe the raw parameter
        self.register_parameter(name="raw_variance", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))
   
        # set the parameter contraint to be positive, when nothing is specified
        if variance_constraint is None:
            variance_constraint = Positive()
            
        # register the constraint
        self.register_constraint("raw_variance", variance_constraint)
        
        #set the parameter prior
        if variance_prior is not None:            
            self.register_prior("variance_prior", variance_prior, lambda m: m.variance, lambda m, v: m._set_variance(v))
        
        if num_dimensions is not None:
            # Remove after 1.0
            warnings.warn("The `num_dimensions` argument is deprecated and no longer used.", DeprecationWarning)
            self.register_parameter(name="offset", parameter=torch.nn.Parameter(torch.zeros(1, 1, num_dimensions)))
        
        if offset_prior is not None:
            # Remove after 1.0
            warnings.warn("The `offset_prior` argument is deprecated and no longer used.", DeprecationWarning)
        self.register_parameter(name="raw_variance", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))
    
      # now set the actual parameter
    @property
    def variance(self):
        return self.raw_variance_constraint.transform(self.raw_variance)
        
    @variance.setter
    def variance(self, value):
        return self._set_variance(value)
        
    def _set_variance(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.raw_variance)
        # transform the actual value to a raw one by applying inverse transform
        self.initialize(raw_variance = self.raw_length_constraint.inverse_transform(value))            
    
    # define the kernel function
    def forward(self, X, X2 = None, last_dim_is_batch=False):
    
        if X2 is None:
            X2 = X
        Xs = torch.sum(torch.square(X), dim = -1)
        X2s = torch.sum(torch.square(X2), dim = -1)
        cross_product = torch.tensordot(X, X2, ([-1], [-1]))
    
        denominator = -cross_product + broadcasting_elementwise(torch.add, Xs, X2s)
    
        return self.variance * cross_product / denominator
 
# ====================GP========================================================
    
# Use the simplest form of GP model, exact inference
class TanimoGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_X, train_y, likelihood):
        super().__init__(train_X, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = TanimotoKernel()

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# initialize likelihood and model
likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint= gpytorch.constraints.GreaterThan(1e-4))
model = TanimoGPModel(train_X, train_y, likelihood)

# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
training_iter = 100
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

for i in range(training_iter):
    # Zero gradients from previous iteration
    optimizer.zero_grad()
    # Output from model
    output = model(torch.Tensor(train_X))
    # Calc loss and backprop gradients
    loss = -mll(output, torch.Tensor(train_y.reshape(-1,)))
    loss.backward()
    print('Iter %d/%d - Loss: %.3f   variance: %.3f   noise: %.3f' % (
        i + 1, training_iter, loss.item(),
        model.covar_module.variance.item(),
        model.likelihood.noise.item()
    ))
    optimizer.step()
 
# =============================================================================    
# gp for predictions
model.eval()
likelihood.eval()


with torch.no_grad():
    observed_pred = likelihood(model(test_X))
    
Z_t = observed_pred.mean.numpy().reshape(-1,1)
y_pred = y_scaler.inverse_transform(Z_t)
y_test1 = y_scaler.inverse_transform(y_test)

score = r2_score(y_test1, y_pred)
rmse = np.sqrt(mean_squared_error(y_test1, y_pred))
mae = mean_absolute_error(y_test1, y_pred)

print("\nR^2: {:.3f}".format(score))
print("RMSE: {:.3f}".format(rmse))
print("MAE: {:.3f}".format(mae))
    


    
